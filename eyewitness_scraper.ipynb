{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c84d3c",
   "metadata": {},
   "source": [
    "# Install & core imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "035061d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q requests beautifulsoup4 reportlab lxml\n",
    "\n",
    "import os, re, sys, time, random, logging, html, textwrap\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import urllib.robotparser as robotparser\n",
    "import dataclasses as dc\n",
    "from typing import List, Iterable, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "# --- ReportLab pieces for PDF ---\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654a647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22bf69a8",
   "metadata": {},
   "source": [
    "# Constants & logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba354a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LIST_URL = \"https://legacy.baseballprospectus.com/prospects/eyewitness.php\"\n",
    "USER_AGENT    = \"EyewitnessScoutBot/0.1 (+https://example.com)\"\n",
    "\n",
    "DEFAULT_DELAY_RANGE = (1.0, 3.0)   # polite crawl delay (seconds)\n",
    "OUTPUT_ROOT         = Path(\"reports\")\n",
    "OUTPUT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s  %(levelname)-8s %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log = logging.getLogger(\"eyewitness\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86aed0b",
   "metadata": {},
   "source": [
    "#  Helpers: session, robots check, dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0999746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RespectfulSession(requests.Session):\n",
    "    \"\"\"Requests session that respects delays & retries.\"\"\"\n",
    "    def __init__(self, delay_range=DEFAULT_DELAY_RANGE):\n",
    "        super().__init__()\n",
    "        self.delay_range = delay_range\n",
    "        self.headers.update({\"User-Agent\": USER_AGENT})\n",
    "        adapter = requests.adapters.HTTPAdapter(\n",
    "            max_retries=requests.adapters.Retry(\n",
    "                total=5, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504]\n",
    "            )\n",
    "        )\n",
    "        self.mount(\"http://\", adapter); self.mount(\"https://\", adapter)\n",
    "\n",
    "    def get(self, url, **kw):\n",
    "        resp = super().get(url, timeout=30, **kw)\n",
    "        time.sleep(random.uniform(*self.delay_range))\n",
    "        return resp\n",
    "\n",
    "\n",
    "def allowed_by_robots(url: str, ua: str = USER_AGENT) -> bool:\n",
    "    parsed = urlparse(url)\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(f\"{parsed.scheme}://{parsed.netloc}/robots.txt\")\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp.can_fetch(ua, url)\n",
    "    except Exception as e:\n",
    "        log.warning(\"robots.txt check failed (%s) – assuming allowed\", e)\n",
    "        return True  # fallback\n",
    "\n",
    "\n",
    "@dc.dataclass(slots=True)\n",
    "class ReportMeta:\n",
    "    url: str\n",
    "    player: str\n",
    "    position: Optional[str] = None\n",
    "    evaluator: Optional[str] = None\n",
    "    report_date: Optional[str] = None\n",
    "    ofp: Optional[str] = None\n",
    "    org: Optional[str] = None\n",
    "    body_html: Optional[str] = None\n",
    "\n",
    "    def pdf_path(self) -> Path:\n",
    "        clean = lambda s: re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s).strip(\"_\")\n",
    "        name  = f\"{clean(self.player)}_{clean(self.org or 'NA')}_{self.report_date or 'undated'}.pdf\"\n",
    "        year  = (self.report_date or \"unknown\").split(\"/\")[-1]\n",
    "        out_dir = OUTPUT_ROOT / year\n",
    "        out_dir.mkdir(exist_ok=True, parents=True)\n",
    "        return out_dir / name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eac412",
   "metadata": {},
   "source": [
    "# Discover report links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e0b099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_report_links(session: requests.Session, limit: Optional[int] = None) -> List[ReportMeta]:\n",
    "    \"\"\"Scrape the index table and return list of ReportMeta (header fields pre-filled).\"\"\"\n",
    "    log.info(\"Fetching index…\")\n",
    "    soup = BeautifulSoup(session.get(BASE_LIST_URL).text, \"lxml\")\n",
    "\n",
    "    links: List[ReportMeta] = []\n",
    "    for a in soup.select(\"a[href^=eyewitness_]\"):\n",
    "        row = a.find_parent(\"tr\")\n",
    "        cells = row.find_all(\"td\") if row else []\n",
    "        links.append(\n",
    "            ReportMeta(\n",
    "                url=urljoin(BASE_LIST_URL, a[\"href\"]),\n",
    "                player=a.get_text(strip=True),\n",
    "                position=cells[1].get_text(strip=True) if len(cells) > 1 else None,\n",
    "                evaluator=cells[2].get_text(strip=True) if len(cells) > 2 else None,\n",
    "                report_date=cells[3].get_text(strip=True) if len(cells) > 3 else None,\n",
    "                ofp=cells[4].get_text(strip=True) if len(cells) > 4 else None,\n",
    "            )\n",
    "        )\n",
    "    log.info(\"Found %d reports\", len(links))\n",
    "    return links[:limit] if limit else links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c51f6",
   "metadata": {},
   "source": [
    "# Fetch details & generate PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PDF helper (ReportLab version) ---\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n",
    "import textwrap, html, re\n",
    "\n",
    "def fetch_report(meta: ReportMeta, session: requests.Session) -> None:\n",
    "    if not allowed_by_robots(meta.url):\n",
    "        log.warning(\"Skipping disallowed URL: %s\", meta.url)\n",
    "        return\n",
    "    soup = BeautifulSoup(session.get(meta.url).text, \"lxml\")\n",
    "\n",
    "    # org from Affiliate line\n",
    "    aff = soup.find(string=re.compile(r\"Affiliate\"))\n",
    "    if aff and (m := re.search(r\"Affiliate\\s+\\([^,]+,\\s*([^)]+)\\)\", aff)):\n",
    "        meta.org = m.group(1).strip()\n",
    "\n",
    "    # crude start-stop extraction\n",
    "    start = soup.find(string=re.compile(re.escape(meta.player)))\n",
    "    body_bits = []\n",
    "    for el in start.parent.next_siblings:  # type: ignore\n",
    "        if isinstance(el, Tag) and el.find(string=re.compile(r\"Terms of Service\")):\n",
    "            break\n",
    "        body_bits.append(str(el))\n",
    "    meta.body_html = \"\".join(body_bits) or soup.prettify()\n",
    "\n",
    "def render_html(meta: ReportMeta) -> str:\n",
    "    header = (\n",
    "        f\"<h1>{html.escape(meta.player)}</h1>\"\n",
    "        f\"<h3>{html.escape(meta.position or '')} | {html.escape(meta.org or '')}</h3>\"\n",
    "        f\"<p><b>Evaluator:</b> {html.escape(meta.evaluator or 'N/A')}<br>\"\n",
    "        f\"<b>Date:</b> {html.escape(meta.report_date or 'N/A')}  \"\n",
    "        f\"<b>OFP:</b> {html.escape(meta.ofp or 'N/A')}</p><hr>\"\n",
    "    )\n",
    "    css = \"\"\"\n",
    "    body { font-family: Helvetica, Arial, sans-serif; margin: 1in; }\n",
    "    h1 { margin: 0; font-size: 26pt; }\n",
    "    h3 { margin-top: .2em; color:#555; }\n",
    "    table { border-collapse: collapse; width:100%; margin-top:1em; }\n",
    "    th,td { border:1px solid #ccc; padding:4px 6px; font-size:10pt; }\n",
    "    th { background:#f0f0f0; }\n",
    "    \"\"\"\n",
    "    return f\"\"\"<!doctype html><html><head><meta charset='utf-8'><style>{css}</style></head>\n",
    "<body>{header}{meta.body_html}</body></html>\"\"\"\n",
    "\n",
    "def strip_tags(raw_html: str) -> str:\n",
    "    \"\"\"Very simple HTML→text converter — good enough for plain reports.\"\"\"\n",
    "    return html.unescape(re.sub(r\"<[^>]+>\", \"\", raw_html).replace(\"\\xa0\", \" \"))\n",
    "\n",
    "def save_pdf(meta: ReportMeta) -> None:\n",
    "    text = strip_tags(render_html(meta))\n",
    "    out_path = meta.pdf_path()\n",
    "\n",
    "    c = canvas.Canvas(str(out_path), pagesize=letter)\n",
    "    width, height = letter\n",
    "    margin = 0.75 * inch\n",
    "    y = height - margin\n",
    "    line_h = 11\n",
    "\n",
    "    for line in textwrap.wrap(text, 95):\n",
    "        if y < margin:          # new page\n",
    "            c.showPage()\n",
    "            y = height - margin\n",
    "        c.drawString(margin, y, line)\n",
    "        y -= line_h\n",
    "\n",
    "    c.save()\n",
    "    log.info(\"✓  %s (ReportLab)\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6891d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, types, certifi\n",
    "\n",
    "def _clean(s: str) -> str:\n",
    "    \"\"\"Keep alphanumerics/underscore, collapse everything else to '_'.\"\"\"\n",
    "    return re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def safe_pdf_path(self):\n",
    "    safe_date = _clean(self.report_date or \"undated\")\n",
    "    name      = f\"{_clean(self.player)}_{_clean(self.org or 'NA')}_{safe_date}.pdf\"\n",
    "    year      = (self.report_date or \"unknown\").split(\"/\")[-1]\n",
    "    out_dir   = OUTPUT_ROOT / year\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir / name\n",
    "\n",
    "# Hot-patch the class defined earlier\n",
    "ReportMeta.pdf_path = types.MethodType(safe_pdf_path, ReportMeta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bd0a260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-09 17:30:21,718  INFO     Fetching index…\n",
      "2025-07-09 17:30:23,074  INFO     Found 1180 reports\n",
      "2025-07-09 17:30:23,202  WARNING  robots.txt check failed (<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>) – assuming allowed\n",
      "2025-07-09 17:30:25,395  ERROR    Failed CJ Abrams: expected string or bytes-like object, got 'member_descriptor'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2863431571.py\", line 9, in <module>\n",
      "    save_pdf(meta)\n",
      "    ~~~~~~~~^^^^^^\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2495931425.py\", line 13, in save_pdf\n",
      "    out_path = meta.pdf_path()\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 8, in safe_pdf_path\n",
      "    safe_date = _clean(self.report_date or \"undated\")\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 5, in _clean\n",
      "    return re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s).strip(\"_\")\n",
      "           ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py\", line 208, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'member_descriptor'\n",
      "2025-07-09 17:30:25,452  WARNING  robots.txt check failed (<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>) – assuming allowed\n",
      "2025-07-09 17:30:27,252  ERROR    Failed Albert Abreu: expected string or bytes-like object, got 'member_descriptor'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2863431571.py\", line 9, in <module>\n",
      "    save_pdf(meta)\n",
      "    ~~~~~~~~^^^^^^\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2495931425.py\", line 13, in save_pdf\n",
      "    out_path = meta.pdf_path()\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 8, in safe_pdf_path\n",
      "    safe_date = _clean(self.report_date or \"undated\")\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 5, in _clean\n",
      "    return re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s).strip(\"_\")\n",
      "           ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py\", line 208, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'member_descriptor'\n",
      "2025-07-09 17:30:27,301  WARNING  robots.txt check failed (<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>) – assuming allowed\n",
      "2025-07-09 17:30:28,917  ERROR    Failed Osvaldo Abreu: expected string or bytes-like object, got 'member_descriptor'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2863431571.py\", line 9, in <module>\n",
      "    save_pdf(meta)\n",
      "    ~~~~~~~~^^^^^^\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2495931425.py\", line 13, in save_pdf\n",
      "    out_path = meta.pdf_path()\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 8, in safe_pdf_path\n",
      "    safe_date = _clean(self.report_date or \"undated\")\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 5, in _clean\n",
      "    return re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s).strip(\"_\")\n",
      "           ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py\", line 208, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'member_descriptor'\n",
      "2025-07-09 17:30:28,969  WARNING  robots.txt check failed (<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>) – assuming allowed\n",
      "2025-07-09 17:30:31,051  ERROR    Failed Albert Abreu: expected string or bytes-like object, got 'member_descriptor'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2863431571.py\", line 9, in <module>\n",
      "    save_pdf(meta)\n",
      "    ~~~~~~~~^^^^^^\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2495931425.py\", line 13, in save_pdf\n",
      "    out_path = meta.pdf_path()\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 8, in safe_pdf_path\n",
      "    safe_date = _clean(self.report_date or \"undated\")\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 5, in _clean\n",
      "    return re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s).strip(\"_\")\n",
      "           ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py\", line 208, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'member_descriptor'\n",
      "2025-07-09 17:30:31,111  WARNING  robots.txt check failed (<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>) – assuming allowed\n",
      "2025-07-09 17:30:33,228  ERROR    Failed Albert Abreu: expected string or bytes-like object, got 'member_descriptor'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2863431571.py\", line 9, in <module>\n",
      "    save_pdf(meta)\n",
      "    ~~~~~~~~^^^^^^\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/2495931425.py\", line 13, in save_pdf\n",
      "    out_path = meta.pdf_path()\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 8, in safe_pdf_path\n",
      "    safe_date = _clean(self.report_date or \"undated\")\n",
      "  File \"/var/folders/qw/rynbsm9n3yvb9l_bjh7rchgh0000gn/T/ipykernel_87241/1328411676.py\", line 5, in _clean\n",
      "    return re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s).strip(\"_\")\n",
      "           ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py\", line 208, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'member_descriptor'\n",
      "\n",
      "Done. PDFs are in /Users/jatinbomrasipeta/Desktop/scoutbot/reports\n"
     ]
    }
   ],
   "source": [
    "# Set how many recent reports to grab (None = all)\n",
    "LATEST = 5\n",
    "\n",
    "session = RespectfulSession()\n",
    "for meta in collect_report_links(session, limit=LATEST):\n",
    "    try:\n",
    "        fetch_report(meta, session)\n",
    "        if meta.body_html:\n",
    "            save_pdf(meta)\n",
    "        else:\n",
    "            log.warning(\"No body for %s\", meta.url)\n",
    "    except Exception as e:\n",
    "        log.exception(\"Failed %s: %s\", meta.player, e)\n",
    "\n",
    "print(f\"\\nDone. PDFs are in {OUTPUT_ROOT.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
